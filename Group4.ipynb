{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khalilbrikTN/Internship_AUCLibraryWebsite_Cross-APIBibliographicDataIntegration/blob/main/Group4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xli2SKHXl2I8"
      },
      "source": [
        "*italicized text*# Load Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MYEz0N6gefSf",
        "outputId": "9ea6ae11-5a22-49e6-8182-9ea4ef817287"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2J2JIMz7kUHA",
        "outputId": "6352ec80-46fa-4b49-cb19-47acfa6b4fd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.0\n",
            "Collecting pymarc\n",
            "  Downloading pymarc-5.2.2-py3-none-any.whl.metadata (12 kB)\n",
            "Downloading pymarc-5.2.2-py3-none-any.whl (158 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymarc\n",
            "Successfully installed pymarc-5.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install xlsxwriter\n",
        "!pip install pymarc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJYLX5nBf96r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VALYa26kg1fz"
      },
      "source": [
        "# Load Data (AUC library Data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60ug_REZodp8"
      },
      "outputs": [],
      "source": [
        "batches_AUC = pd.read_csv('/content/drive/MyDrive/GROUP 4 -- Metadata/Codes/barches_AUC.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXO-USWili43"
      },
      "source": [
        "# Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbbsdrATuxou"
      },
      "source": [
        "# Load HathiTrust Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjZxw0hcdsUB"
      },
      "outputs": [],
      "source": [
        "marc_xml_found_count = 0\n",
        "\n",
        "# Helper Functions\n",
        "\n",
        "def cleaner(value):\n",
        "    \"\"\"\n",
        "    Extract and return the first sequence of digits from the input string.\n",
        "\n",
        "    Parameters:\n",
        "    value (str): The input string that may contain digits along with other characters.\n",
        "\n",
        "    Returns:\n",
        "    str or None: The first sequence of digits found in the string, or None if no digits are found.\n",
        "    \"\"\"\n",
        "    # Use regular expression to find all sequences of digits in the string\n",
        "    matches = re.findall(r'\\d+', str(value))\n",
        "    # Return the first sequence of digits found, or None if no digits are found\n",
        "    return matches[0] if matches else value\n",
        "\n",
        "\n",
        "def fetch_hathi_data(id_type, id_value):\n",
        "    \"\"\"\n",
        "    Fetch data from HathiTrust API based on the given identifier type and value.\n",
        "\n",
        "    Parameters:\n",
        "    id_type (str): The type of identifier (e.g., 'oclc', 'lccn', 'isbn').\n",
        "    id_value (str): The value of the identifier.\n",
        "\n",
        "    Returns:\n",
        "    dict: The JSON response from the HathiTrust API or None if the request fails.\n",
        "    \"\"\"\n",
        "    url = f\"https://catalog.hathitrust.org/api/volumes/full/{id_type}/{id_value}.json\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def extract_record_data(record_data):\n",
        "    \"\"\"\n",
        "    Extract relevant data from a HathiTrust record.\n",
        "\n",
        "    Parameters:\n",
        "    record_data (dict): The HathiTrust record data.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the title and MARC XML of the record.\n",
        "    \"\"\"\n",
        "    title_Hathi = record_data['titles'][0] if 'titles' in record_data else ''\n",
        "    marc_xml = record_data['marc-xml'] if 'marc-xml' in record_data else ''\n",
        "    return title_Hathi, marc_xml\n",
        "\n",
        "def extract_item_data(item, record_id):\n",
        "    \"\"\"\n",
        "    Extract relevant data from a HathiTrust item.\n",
        "\n",
        "    Parameters:\n",
        "    item (dict): The HathiTrust item data.\n",
        "    record_id (str): The ID of the record the item belongs to.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the view status and item URL of the record, or None if the item does not match the record ID.\n",
        "    \"\"\"\n",
        "    if item['fromRecord'] == record_id:\n",
        "        view_status = item['usRightsString']\n",
        "        record_url = item['itemURL']\n",
        "        return view_status, record_url\n",
        "    return None\n",
        "\n",
        "def create_result(AUC_title, title_Hathi, author, Publisher, datePublish, AUC_Number, cleaned_oclc, cleaned_lccn, cleaned_isbn, marc_xml, record_url, view_status, id_type, found_id):\n",
        "    global marc_xml_found_count\n",
        "\n",
        "    if marc_xml != '':\n",
        "          marc_xml_found_count = marc_xml_found_count + 1\n",
        "\n",
        "    return {\n",
        "        'AUC_Title': AUC_title,\n",
        "        'Hathi_Title': title_Hathi,\n",
        "        'Author': author,\n",
        "        'Published': Publisher,\n",
        "        'D.O. Pub.': datePublish,\n",
        "        'AUC no.': AUC_Number ,\n",
        "        'OCLC': cleaned_oclc,\n",
        "        'LCCN': cleaned_lccn,\n",
        "        'ISBN': cleaned_isbn,\n",
        "        'MARC XML': marc_xml,\n",
        "        'Full View URL': record_url,\n",
        "        'View': view_status,\n",
        "        #'No ID provided' if not found_id else 'No matching record',\n",
        "        'ID Used': id_type\n",
        "    }\n",
        "\n",
        "\n",
        "def process_identifiers(row, identifiers):\n",
        "\n",
        "    found_id = False\n",
        "    record_added = False\n",
        "\n",
        "    AUC_title = row['TITLE']\n",
        "    cleaned_oclc = identifiers.get('oclc')\n",
        "    cleaned_lccn = identifiers.get('lccn')\n",
        "    cleaned_isbn = identifiers.get('isbn')\n",
        "    author = row['AUTHOR']\n",
        "    Publisher = row['PUBLISHED']\n",
        "    datePublish = row['D.O. Pub.']\n",
        "    AUC_Number = row['AUC no.']\n",
        "\n",
        "\n",
        "    for id_type, id_value in identifiers.items():\n",
        "        if pd.notna(id_value) and id_value:\n",
        "            found_id = True\n",
        "            json_data = fetch_hathi_data(id_type, id_value)\n",
        "            if json_data:\n",
        "                for record_id, record_data in json_data['records'].items():\n",
        "                    title_Hathi, marc_xml = extract_record_data(record_data)\n",
        "\n",
        "                    for item in json_data['items']:\n",
        "                        item_data = extract_item_data(item, record_id)\n",
        "                        if item_data:\n",
        "                            view_status, record_url = item_data\n",
        "\n",
        "                            return create_result(AUC_title, title_Hathi, author, Publisher, datePublish, AUC_Number, cleaned_oclc, cleaned_lccn, cleaned_isbn, marc_xml, record_url, view_status, id_type, found_id)\n",
        "\n",
        "\n",
        "    view_status = 'No ID provided' if not found_id else 'No matching record'\n",
        "\n",
        "    return create_result(AUC_title, 'None', author, Publisher, datePublish, AUC_Number, cleaned_oclc, cleaned_lccn, cleaned_isbn, '', '', view_status, 'None', found_id)\n",
        "\n",
        "\n",
        "\n",
        "def process_row(row):\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Process a single row of the DataFrame to fetch and extract data from HathiTrust.\n",
        "\n",
        "    Parameters:\n",
        "    row (pd.Series): The original row of data.\n",
        "\n",
        "    Returns:\n",
        "    dict: A dictionary containing the result data.\n",
        "    \"\"\"\n",
        "    identifiers = {\n",
        "        'oclc': cleaner(row['OCLC no.']),\n",
        "        'lccn': cleaner(row['LC no.']),\n",
        "        'isbn': cleaner(row['ISBN'])\n",
        "    }\n",
        "\n",
        "    return process_identifiers(row, identifiers)\n",
        "\n",
        "\n",
        "\n",
        "def process_dataframe(df, n):\n",
        "\n",
        "    results = []\n",
        "    row_num = 0\n",
        "    for index, row in df.head(n).iterrows():\n",
        "        new_row = process_row(row)\n",
        "        results.append(new_row)\n",
        "        row_num = row_num + 1\n",
        "\n",
        "    return results\n",
        "\n",
        "def save_results_to_csv(results_df, output_path):\n",
        "    \"\"\"\n",
        "    Save the results to a CSV file.\n",
        "    \"\"\"\n",
        "\n",
        "    results_df.to_csv(output_path, index=False)\n",
        "\n",
        "\n",
        "def matching_stats(df):\n",
        "    \"\"\"\n",
        "    Calculate and print matching statistics from the DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    df (pd.DataFrame): The DataFrame containing the result data.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing counts of empty MARC XML, empty View URLs, and ID usage statistics.\n",
        "    \"\"\"\n",
        "    # Initialize counters\n",
        "    empty_marc_count = 0\n",
        "    found_marc_count = 0\n",
        "    oclc_found_count = 0\n",
        "    lccn_found_count = 0\n",
        "    isbn_found_count = 0\n",
        "    # Iterate through each row in the DataFrame\n",
        "    for index, row in df.iterrows():\n",
        "        value_marc = row['MARC XML']\n",
        "        if value_marc is None or value_marc == '':\n",
        "            empty_marc_count += 1\n",
        "        else:\n",
        "            found_marc_count += 1\n",
        "            if pd.notna(row['OCLC']):\n",
        "                oclc_found_count += 1\n",
        "            if pd.notna(row['LCCN']):\n",
        "                lccn_found_count += 1\n",
        "            if pd.notna(row['ISBN']):\n",
        "                isbn_found_count += 1\n",
        "\n",
        "\n",
        "\n",
        "    # Print the results\n",
        "    print('Records Processed:', len(df))\n",
        "    print('Records with NO MARC XML found:', empty_marc_count)\n",
        "    print('Records with MARC XML found:', found_marc_count)\n",
        "    print('Records with MARC XML found where OCLC is not empty:', oclc_found_count)\n",
        "    print('Records with MARC XML found where lccn is not empty:', lccn_found_count)\n",
        "    print('Records with MARC XML found where ISBN is not empty:', isbn_found_count)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xp3inlGcwIQ2"
      },
      "source": [
        "# ***Batch1 : Process with Hathi***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4koP4zpZwL_X"
      },
      "outputs": [],
      "source": [
        "updatedBatch1 = pd.read_excel('/content/drive/MyDrive/GROUP 4 -- Metadata/Metadata Batches/NEW 18 7 2024/Metadata for Group 4-1.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JqIX43rATc5"
      },
      "outputs": [],
      "source": [
        "sub_batches = []\n",
        "\n",
        "sub_batches.append(updatedBatch1.iloc[0:10000])\n",
        "sub_batches.append(updatedBatch1.iloc[10000:20000])\n",
        "sub_batches.append(updatedBatch1.iloc[20000:30000])\n",
        "sub_batches.append(updatedBatch1.iloc[30000:40000])\n",
        "sub_batches.append(updatedBatch1.iloc[40000:50000])\n",
        "sub_batches.append(updatedBatch1.iloc[50000:60001])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raT46v87AjCQ"
      },
      "outputs": [],
      "source": [
        "counter = 1\n",
        "\n",
        "for batch in sub_batches:\n",
        "\n",
        "  HATHI_batch = pd.DataFrame(process_dataframe(batch, len(batch)))\n",
        "  save_results_to_excel(HATHI_batch, f'/content/drive/MyDrive/GROUP 4 -- Metadata/Codes/Output/sub{counter}.xlsx')\n",
        "  counter = counter + 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgG9flVc6SLh"
      },
      "source": [
        "### Concatenate results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YD9-xMKOk_uY"
      },
      "outputs": [],
      "source": [
        "sub1 = pd.read_excel('/content/drive/MyDrive/GROUP 4 -- Metadata/Codes/Output/sub1.xlsx')\n",
        "sub2 = pd.read_excel('/content/drive/MyDrive/GROUP 4 -- Metadata/Codes/Output/sub2.xlsx')\n",
        "sub3 = pd.read_excel('/content/drive/MyDrive/GROUP 4 -- Metadata/Codes/Output/sub3.xlsx')\n",
        "sub4 = pd.read_excel('/content/drive/MyDrive/GROUP 4 -- Metadata/Codes/Output/sub4.xlsx')\n",
        "sub5 = pd.read_excel('/content/drive/MyDrive/GROUP 4 -- Metadata/Codes/Output/sub5.xlsx')\n",
        "sub6 = pd.read_excel('/content/drive/MyDrive/GROUP 4 -- Metadata/Codes/Output/sub6.xlsx')\n",
        "\n",
        "# Concatenate all DataFrames into a single DataFrame\n",
        "concatenated_df = pd.concat([sub1, sub2, sub3, sub4, sub5, sub6], ignore_index=True)\n",
        "\n",
        "# Save the concatenated DataFrame to a CSV file\n",
        "output_path = '/content/drive/MyDrive/GROUP 4 -- Metadata/Codes/Output/concatenated_output.csv'\n",
        "concatenated_df.to_csv(output_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgEzZxY2_ICd"
      },
      "source": [
        "# Experiment with Open Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tz9mLFqNdkWi"
      },
      "outputs": [],
      "source": [
        "def fetch_openlibrary_data(id_type, id_value):\n",
        "    base_url = 'https://openlibrary.org/api/books'\n",
        "    url = f'{base_url}?bibkeys={id_type.upper()}:{id_value}&format=json&jscmd=data'\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        print(response.json())\n",
        "        return response.json()\n",
        "    return None\n",
        "\n",
        "def extract_openlibrary_record_data(record_data):\n",
        "    title = record_data.get('title', 'None')\n",
        "    view_link = record_data.get('url', None)\n",
        "\n",
        "\n",
        "    preview_link = record_data.get('ebooks', None)\n",
        "\n",
        "\n",
        "    identifiers = {\n",
        "        'oclc': None,\n",
        "        'lccn': None,\n",
        "        'isbn': None\n",
        "    }\n",
        "    if 'identifiers' in record_data:\n",
        "        identifiers['oclc'] = record_data['identifiers'].get('oclc', [None])[0]\n",
        "        identifiers['lccn'] = record_data['identifiers'].get('lccn', [None])[0]\n",
        "        identifiers['isbn'] = record_data['identifiers'].get('isbn_10', [None])[0] or record_data['identifiers'].get('isbn_13', [None])[0]\n",
        "\n",
        "    return title, view_link, identifiers\n",
        "\n",
        "def process_row_for_openlibrary(row):\n",
        "    identifiers = {\n",
        "        'oclc': cleaner(row['OCLC']),\n",
        "        'lccn': cleaner(row['LCCN']),\n",
        "        'isbn': cleaner(row['ISBN'])\n",
        "    }\n",
        "\n",
        "    AUC_title = row['AUC_Title']\n",
        "    HATH_title = row['Hathi_Title']\n",
        "    author = row['Author']\n",
        "    Publisher = row['Published']\n",
        "    datePublish = row['D.O. Pub.']\n",
        "    AUC_Number = row['AUC no.']\n",
        "    xml_HATHI = row['MARC XML']\n",
        "    Full_View_URL_HATHI = row['Full View URL (HATHI)']\n",
        "    View_HATHI= row['View (HATHI)']\n",
        "    ID_HATHI = row['ID Used (HATHI)']\n",
        "    openlibrary_title = 'None'\n",
        "    view_link = ''\n",
        "    found_id = False\n",
        "\n",
        "    for id_type, id_value in identifiers.items():\n",
        "        if pd.notna(id_value) and id_value:\n",
        "            json_data = fetch_openlibrary_data(id_type, id_value)\n",
        "            if json_data:\n",
        "                key = f'{id_type.upper()}:{id_value}'\n",
        "                if key in json_data:\n",
        "                    record_data = json_data[key]\n",
        "                    openlibrary_title, view_link, new_identifiers = extract_openlibrary_record_data(record_data)\n",
        "                    for k, v in new_identifiers.items():\n",
        "                        if not identifiers[k] and v:\n",
        "                            identifiers[k] = v\n",
        "                    found_id = True\n",
        "                    break\n",
        "\n",
        "    return {\n",
        "        'AUC_Title': AUC_title,\n",
        "        'Hathi_Title': HATH_title,\n",
        "        'OpenLibrary_Title': openlibrary_title,\n",
        "        'Author': author,\n",
        "        'Published': Publisher,\n",
        "        'D.O. Pub.': datePublish,\n",
        "        'OCLC': identifiers['oclc'],\n",
        "        'LCCN': identifiers['lccn'],\n",
        "        'ISBN': identifiers['isbn'],\n",
        "        'MARC XML(HATHI)' : xml_HATHI,\n",
        "        'Full View URL (HATHI)' : Full_View_URL_HATHI,\n",
        "        'View (HATHI)' : View_HATHI,\n",
        "        'ID Used (HATHI)' : ID_HATHI,\n",
        "        'View Link (OpenLibrary)': view_link,\n",
        "        'Found_OpenLibrary': 'Yes' if found_id else 'No',\n",
        "        'AUC no.': AUC_Number\n",
        "    }\n",
        "\n",
        "def process_dataframe(df, n):\n",
        "\n",
        "    results = []\n",
        "    row_num = 0\n",
        "    for index, row in df.head(n).iterrows():\n",
        "        new_row = process_row_for_openlibrary(row)\n",
        "        results.append(new_row)\n",
        "        row_num = row_num + 1\n",
        "\n",
        "    return results\n",
        "\n",
        "def save_results_to_excel(results_df, output_path):\n",
        "    \"\"\"\n",
        "    Save the results to an Excel file.\n",
        "    \"\"\"\n",
        "\n",
        "    results_df.to_excel(output_path, index=False, engine='xlsxwriter')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECTihXHVh74x"
      },
      "outputs": [],
      "source": [
        "HATHI_OUT_1 = pd.read_csv('/content/drive/MyDrive/GROUP 4 -- Metadata/Codes/Output/BATCH1OUT/HATHIOUT/concatenated_output.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HKgrRjpDiKR",
        "outputId": "ae756bb5-0bf9-4b36-d51b-f2a8b6f3405d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "source": [
        "len(HATHI_OUT_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uNZDrKF7Dkcr",
        "outputId": "c6d6d59a-1fd9-419e-d988-eaf21de53caf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://archive.org/details/lostmenofamerica00holb\n",
            "None\n",
            "None\n",
            "None\n",
            "https://archive.org/details/twelvestoriesbyf0000unse\n",
            "None\n",
            "None\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "df_combined = pd.DataFrame(process_dataframe(HATHI_OUT_1, 10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6AC4ZZuD_XQ"
      },
      "outputs": [],
      "source": [
        "df_combined.head()\n",
        "\n",
        "df_combined.to_excel('/content/drive/MyDrive/GROUP 4 -- Metadata/Codes/Output/tmp.xlsx', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5Vn5JF_EQPI"
      },
      "outputs": [],
      "source": [
        "sub_batches = []\n",
        "\n",
        "sub_batches.append(HATHI_OUT_1.iloc[0:10000])\n",
        "sub_batches.append(HATHI_OUT_1.iloc[10000:20000])\n",
        "sub_batches.append(HATHI_OUT_1.iloc[20000:30000])\n",
        "sub_batches.append(HATHI_OUT_1.iloc[30000:40000])\n",
        "sub_batches.append(HATHI_OUT_1.iloc[40000:50000])\n",
        "sub_batches.append(HATHI_OUT_1.iloc[50000:60001])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counter = 1\n",
        "\n",
        "for batch in sub_batches:\n",
        "\n",
        "  OPEN_BATCH = pd.DataFrame(process_dataframe(batch, len(batch)))\n",
        "  save_results_to_excel(OPEN_BATCH, f'/content/drive/MyDrive/GROUP 4 -- Metadata/Codes/Output/BATCH1OUT/OPENOUT/sub{counter}.xlsx')\n",
        "  counter = counter + 1\n",
        "  print(counter)\n"
      ],
      "metadata": {
        "id": "IBXc3jsRUbhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "domrvKOoVEbu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}